
第3章 线性模型

本章将讲述线性模型中的线性回归模型和 logistic 模型。通常，线性回归模型用于处理回归问题，logistic 模型用于处理分类问题。我们将学习线性模型的数学表示和原理并使用梯度下降法求解模型参数。线性模型本身具有广泛用途，更是深度学习模型的基石。学习并理解相对简单的线性模型有助于学习复杂的深度学习模型。如果你对线性模型不是很熟悉，可以认真学习本章，打好学习深度学习的基础:如果你熟悉线性模型，也请浏览本章内容，因为本章将介绍深度学习中也会用到的术语、符号和优化算法(梯度下降法)。

第4章深度神经网络
前面章节介绍了线性代数、微积分和概率论的相关知识，以及Anaconda、Jupyter Notebook和 Python 语言的相关内容，并且深入讲解了线性回归模型、logistic 模型和梯度下降法。本章将正式开始介绍深度神经网络。本章将从以下4个方面来介绍。
为什么需要深度神经网络?
正向传播算法:使用深度神经网络得到预测值的方法
反向传播算法:计算深度神经网络参数梯度的方法。
深度神经网络的完整训练流程:结合正向传播算法和反向传播算法，使用梯度下降法训练深度神经网络。



激活函数第5章
在神经网络中,隐藏层和输出层的节点都表示其上一层节点的加权和代入激活函数得到的函数值。例如，在图 5-1所示的神经网络中，s是1,x，x的加权和，把s代入 sigmoid 函数，得到h;同样地，s是1，x，x的加权和，把s代入 sigmoid 函数，得到ん。得到隐藏层h，h之后，s是1，h，h的加权和，把s代入 sigmoid 函数，得到输出值。在神经网络中，sigmoid 函数称为激活函数。
输入层
隐藏层输出层图 5-1 激活函数为 sigmoid 函数的神经网络
在实际应用中，激活函数不仅包括 sigmoid 函数，还有很多其他函数。本章将介绍两个应用于输出层的激活函数--sigmoid 函数、softmax 函数,以及4个常应用于隐藏层的激活函数一sigmoid 函数、tanh 函数、ReLU 函数、Leaky ReLU 函数



第6章模型评估和正则化
现在我们已经知道如何建立一个基本的神经网络模型。在本章中，我们将学习评估神经网络模型的方法，讨论神经网络模型经常会出现的两个问题--欠拟合和过拟合。


第7章 基于TensorFlow 2建立深度学习模型
通过对前面几章的学习，我们已经学会了如何使用 Python 的基本函数(包括 NumPy 的函数)建立神经网络模型，如何使用正向传播算法、反向传播算法及梯度下降法训练神经网络,如何评估神经网络模型的模型误差及控制过拟合。然而，当处理复杂问题时，建立的神经网络模型需要更加复杂的结构和多样化的控制过拟合的方法。这时，每次建立神经网络模型都从最基础的 Python 语句开始会非常困难，容易出错，而且运行效率低。因此，大型科技公司或者高校研究团队开发了深度学习框架，旨在提高深度学习的应用效率。目前较流行的深度学习框架有TensorFlow、PyTorch、Caffe、Theano、MXNet 等。
TensorFlow 是一个开源深度学习框架，由 Google 推出并且维护，拥有较大的用户群体和社区。TensorFlow的优点如下，
易用性。TensorFlow 提供大量容易理解且可读性强的函数，使得它的工作流程相对容
易，兼容性好。例如，TensorFlow 可以很好地与NumPy结合，让数据科学家更容易理解和使用。灵活性。TensorFlow 能够在各种类型的设备上运行，从手机到超级计算机。TensorFlow可以很好地支持分布式计算，可以同时在多个CPU 或者 GPU 上运行。高效性。TensorFlow 在计算资源要求高的部分采用 C++编写，且开发团队花费了大量时间和精力来改进 TensorFlow 的大部分代码。随着越来越多开发人员的努力，TensorFlow 的运行效率不断提高。
具有良好的支持:TensorFlow有Google 团队支持:Google 在自己的日常工作中也使用 TensorFlow，并且持续对其提供支持，在 TensorFlow 周围形成了一个强大社区。
本章将介绍如何使用 TensorFlow2 建立深度学习模型:



第8章卷积神经网络
到现在为止，我们建立的神经网络都是全连接的。全连接的意思是相邻两层之间的节点都相连，如图 8-1 所示。
全连接神经网络可以处理很多一般问题。然而，当数据自变量的结构达到一定复杂的程度时，全连接神经网络通常表现一般。例如，在图8-2 所示的图片中，黑色矩形框中的像素组成了 Lena 的右眼。这些像素具备特定的结构，例如，中间是黑色的瞳孔，四周是眼白，像素只有这样排列才能构成眼睛。换句话说，这些像素具有内在关联，是相关的。在实际中，全连接神经网络不能很好地处理图片数据，以及自变量具有一定复杂结构的类似数据。
这时我们需要建立卷积神经网络(Convolution Neu ralNetwork，CNN)。卷积神经网络可以简单有效地处理自变量的内在结构，是深度学习在计算机视觉领域实现优异表现的基石。本章将首先介绍卷积神经网络两个重要的组成部分--卷积(convolution)和池化(pooling)，然后把卷积和池化应用到 MNIST 数据集中，建立卷积神经网络，使得手写数字图片的分类达到



第9章 基于 TensorFlow 2搭建卷积神经网络模型
在实践中，卷积神经网络可能包括多个卷积层/池化层或者全连接层，使得卷积神经网络包含几十甚至几百个隐藏层。如果我们使用 NumPy 等实现卷积神经网络，则我们需要把很多精力放在正向传播算法、反向传播算法和梯度下降法等技术细节上。使用深度学习框架可以让我们集中精力于数据预处理、神经网络的结构设计及超参数的选择等方面，从而提高建模效率和算法运行效率。本章主要介绍以下内容。
(1)TensorFlow2 中实现卷积层和池化层的函数。
(2)使用 TensorFlow2分析MNIST 数据集(黑白图片)和 CIFAR-10 数据集(彩色图片)。在计算机视觉中，黑白图片通常使用灰度值表示成二维数组的形式。图片的维度涉及宽度(W，width)和高度(H，Height)。
彩色图片通常用 R、G、B 这3种颜色表示。彩色图片的每一个像素都由R(红色)、G(绿色)、B(蓝色)3 个数值表示，如图9-1所示。R、G、B 的数值范围都是0~255。
彩色图片除图片宽度(W)和高度(H)之外，还有深度(也称为颜色通道(channel)，用C表示)。因此，彩色图片用三维数组表示。卷积神经网络的建模技巧。


第10章 循环神经网络
在前面的章节中，我们学习了普通神经网络和卷积神经网络。在一般情况下，普通神经网络通过增加隐藏层与控制过拟合，在很多实际问题中可以取得很好的预测效果。卷积神经网络通过加入卷积层、池化层等提高了对图片等结构数据的处理能力。然而，当处理具有序列特征的数据(序列数据)时，普通神经网络和卷积神经网络都无法取得很好的效果。这是因为普通神经网络和卷积神经网络都无法有效地考虑序列数据观测点之间的关系。然而，序列数据广泛存在于日常生活中，如天气预报、经济分析等。因此，循环神经网络(Recurrent Neural Network，RNN)便应运而生了。在现实生活中，循环神经网络在很多领域如机器翻译、语音识别、自动文本归纳、手势识别及步态识别等)获得了极大成功。在本章及本书后面的童节中，我们将基于自然语言处理问题，陆续学习循环神经网络的原理和应用。


第 11章搭建深度学习框架
在本章中，我们将自己搭建一个深度学习框架。除深度学习框架 TensorFlow 之外，还有其他性能优良的深度学习框架。为什么要自己搭建一个深度学习框架呢?原因有两点。
(1)搭建一个深度学习框架，有助于我们更加深入地了解深度学习的原理和实现过程，如自动求导和梯度下降法等。虽然自建的深度学习框架与 TensorFlow、PyTorch 等有很大不同，但是这依然可以帮助我们了解这些框架的特性，加深对已有深度学习框架的理解。
(2)我们将在第 12 章学习两种常用的循环神经网络--LSTM 和 GRU。这两种模型的结构比较复杂，计算过程也比较复杂。在自建框架下，我们可以更方便从零实现，且更好地理解LSTM 网络和 GRU。



第12章 长短期记忆模型LSTM与门控循环单元模型GRU
循环神经网络在实际应用(如机器翻译、语音转文字及其他序列数据处理)中已经展现出了优异效果。然而，工程师很少通过建立简单循环神经网络模型达到好的效果。当输入序列较长时，简单循环神经网络模型容易出现梯度消失或者梯度爆炸问题，这使得简单循环神经网络模型在实际应用中较难捕捉序列中距离较大的依赖关系。
本章将探索简单循环神经网络模型的主要缺陷--梯度消失或者梯度爆炸，并介绍实际应用中经典的改良循环神经网络模型--长短期记忆(Long Short Term Memory,LSTM)模型，以及越来越流行的循环神经网络模型--门控循环单元(Gated Recurrent Unit，GRU)。


第13章 基于 TensorFlow 2搭建循环神经网络模型
在前面的章节中，我们学习了简单循环神经网络模型，以及两种特殊结构的循环神经网络模型--LSTM 模型和 GRU 模型。另外，我们自己搭建了一个深度学习框架，基于深度学习框架，可以轻松地从零实现简单循环神经网络、LSTM 模型和 GRU 模型。不过，自建框架只适合研究深度学习，而不能作为实际应用的工具。基于 TensorFlow 框架，我们将在本章学习在实际工作中建立循环神经网络模型的方式。在本章中，我们主要的任务如下:
建立 LSTM 模型，分析 IMDB 中的数据:
建立 GRU 模型，像莎士比亚一样写作。


本书主要内客
线性模型、激活函数:
深度神经囤络，卷积神经网络、循环神经网络:
模型评估和正则化
搭建深度学习框架:
基于TensorFow2搭建卷积神经网络模型:
基于TensorHlow2搭建循环神经网络模型



# 深度学习基础教案

根据课本内容，以下是一个基础的深度学习教案总结概括：

## 第一部分：基础理论与模型

### 第1-2章：数学基础与编程环境
- 线性代数、微积分和概率论基础知识
- Anaconda、Jupyter Notebook和Python语言基础
- 数据科学编程环境搭建

### 第3章：线性模型
- 线性回归模型（回归问题）
- Logistic模型（分类问题）
- 梯度下降法求解模型参数
- 线性模型作为深度学习基石的重要性

### 第4章：深度神经网络
- 深度神经网络的必要性
- 正向传播算法：预测值计算
- 反向传播算法：参数梯度计算
- 完整训练流程：结合正反向传播与梯度下降

### 第5章：激活函数
- 输出层激活函数：sigmoid函数、softmax函数
- 隐藏层激活函数：sigmoid函数、tanh函数、ReLU函数、Leaky ReLU函数
- 激活函数在神经网络中的作用与选择

### 第6章：模型评估和正则化
- 神经网络模型评估方法
- 欠拟合与过拟合问题分析
- 正则化技术与实践

## 第二部分：深度学习框架与高级模型

### 第7章：基于TensorFlow 2建立深度学习模型
- TensorFlow框架介绍与优势
- TensorFlow的易用性、灵活性、高效性与社区支持
- 使用TensorFlow 2构建基础深度学习模型

### 第8章：卷积神经网络
- 全连接网络的局限性
- 卷积神经网络基本原理
- 卷积操作与池化操作
- MNIST数据集上的CNN应用

### 第9章：基于TensorFlow 2搭建卷积神经网络模型
- TensorFlow中的卷积层和池化层实现
- 黑白图片(MNIST)与彩色图片(CIFAR-10)处理
- 图像数据的表示与处理
- 卷积神经网络的建模技巧

### 第10章：循环神经网络
- 序列数据处理的挑战
- 循环神经网络基本原理
- RNN在自然语言处理中的应用
- 循环神经网络的优势与局限

### 第11章：搭建深度学习框架
- 深度学习框架原理探究
- 自动求导与梯度下降法实现
- 为理解复杂模型打基础

### 第12章：长短期记忆模型与门控循环单元模型
- 简单RNN的梯度消失/爆炸问题
- LSTM模型结构与原理
- GRU模型结构与原理
- 长序列依赖关系的捕捉

### 第13章：基于TensorFlow 2搭建循环神经网络模型
- 实际工作中的RNN模型构建
- LSTM模型分析IMDB数据
- GRU模型文本生成应用

## 教学建议

1. **循序渐进**：从线性模型开始，逐步过渡到复杂的深度学习模型
2. **理论结合实践**：每个理论概念后安排编程实践
3. **项目驱动**：围绕MNIST、CIFAR-10、IMDB等经典数据集设计项目
4. **框架应用**：先理解原理，再学习框架应用，最后进行实际项目开发
5. **分阶段评估**：基础模型、CNN模型、RNN模型三个阶段分别进行评估

## 学习路径

1. 数学基础 → 线性模型 → 深度神经网络 → 激活函数 → 模型评估
2. TensorFlow基础 → CNN原理 → TensorFlow CNN实现
3. RNN原理 → 高级RNN模型(LSTM/GRU) → TensorFlow RNN实现
4. 综合项目：结合CNN与RNN的实际应用开发

此教案可根据学生基础和课程时长进行适当调整，重点应放在深度学习的核心概念和实际应用能力的培养上。