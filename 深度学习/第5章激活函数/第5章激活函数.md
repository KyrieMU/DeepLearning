
第5章 激活函数 

# 1. 激活函数概述

在神经网络中，隐藏层和输出层的节点都表示其上一层节点的加权和代入激活函数得到的函数值。例如，在图 5-1所示的神经网络中，s₁是1，x₁，x₂的加权和，把s₁代入 sigmoid 函数，得到h₁；同样地，s₂是1，x₁，x₂的加权和，把s₂代入 sigmoid 函数，得到h₂。得到隐藏层h₁，h₂之后，s₃是1，h₁，h₂的加权和，把s₃代入 sigmoid 函数，得到输出值。在神经网络中，sigmoid 函数称为激活函数。

![图 5-1 激活函数为 sigmoid 函数的神经网络](./neural_network_sigmoid.png)

在实际应用中，激活函数不仅包括 sigmoid 函数，还有很多其他函数。本章将介绍两个应用于输出层的激活函数--sigmoid 函数、softmax 函数，以及4个常应用于隐藏层的激活函数一sigmoid 函数、tanh 函数、ReLU 函数、Leaky ReLU 函数。

# 2. 激活函数的数学原理

激活函数的主要作用是引入非线性因素，使神经网络能够学习和模拟复杂的函数关系。如果没有激活函数（或使用线性激活函数），无论神经网络有多少层，输出都只是输入的线性组合，无法解决非线性问题。

## 2.1 数学表达式

假设神经元接收到的加权输入为 z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b，其中 w 是权重，x 是输入，b 是偏置项。激活函数 f 作用于 z，产生神经元的输出 a = f(z)。

不同激活函数的数学表达式如下：

1. **Sigmoid 函数**：
   - 公式：σ(z) = 1 / (1 + e^(-z))
   - 值域：(0, 1)
   - 导数：σ'(z) = σ(z)(1 - σ(z))

2. **Tanh 函数**：
   - 公式：tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
   - 值域：(-1, 1)
   - 导数：tanh'(z) = 1 - tanh²(z)

3. **ReLU 函数**：
   - 公式：ReLU(z) = max(0, z)
   - 值域：[0, +∞)
   - 导数：ReLU'(z) = 1 if z > 0 else 0

4. **Leaky ReLU 函数**：
   - 公式：LeakyReLU(z) = max(αz, z)，其中 α 是一个小正数（如0.01）
   - 值域：(-∞, +∞)
   - 导数：LeakyReLU'(z) = 1 if z > 0 else α

5. **Softmax 函数**：
   - 公式：softmax(z)ᵢ = e^(zᵢ) / Σⱼ e^(zⱼ)
   - 值域：(0, 1)，且所有输出之和为1
   - 导数：复杂，涉及到雅可比矩阵

# 3. 激活函数的特性与应用

## 3.1 Sigmoid 函数

**特性**：
- 平滑、可微
- 输出范围有限，适合二分类问题
- 存在梯度消失问题
- 输出不以零为中心

**应用**：
- 二分类问题的输出层
- 早期神经网络的隐藏层（现已较少使用）

## 3.2 Tanh 函数

**特性**：
- 与Sigmoid类似，但输出以零为中心
- 仍存在梯度消失问题
- 通常比Sigmoid表现更好

**应用**：
- 隐藏层激活函数
- 循环神经网络（RNN）

## 3.3 ReLU 函数

**特性**：
- 计算简单，训练速度快
- 缓解梯度消失问题
- 存在"死亡ReLU"问题（神经元可能永久失活）
- 输出不以零为中心

**应用**：
- 深度卷积神经网络的隐藏层
- 目前最常用的激活函数

## 3.4 Leaky ReLU 函数

**特性**：
- 解决了"死亡ReLU"问题
- 保留了ReLU的大部分优点
- 计算略微复杂

**应用**：
- 需要避免神经元死亡的深度网络

## 3.5 Softmax 函数

**特性**：
- 将多个值转换为概率分布
- 所有输出之和为1
- 强调最大值，抑制其他值

**应用**：
- 多分类问题的输出层

# 4. 激活函数的代码实现与可视化

下面我们使用Python实现各种激活函数，并通过matplotlib进行可视化：

```python
import numpy as np
import matplotlib.pyplot as plt

# 设置中文显示
plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体
plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题

# 定义激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # 减去最大值以提高数值稳定性
    return exp_x / exp_x.sum()

# 定义导数函数
def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh_derivative(x):
    t = tanh(x)
    return 1 - t**2

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)

# 创建输入数据
x = np.linspace(-10, 10, 1000)
x_softmax = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])  # Softmax示例输入

# 创建图形
fig, axs = plt.subplots(3, 2, figsize=(15, 15))

# 绘制激活函数
axs[0, 0].plot(x, sigmoid(x), 'r-', linewidth=2)
axs[0, 0].set_title('Sigmoid 函数')
axs[0, 0].grid(True)
axs[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axs[0, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)

axs[0, 1].plot(x, tanh(x), 'g-', linewidth=2)
axs[0, 1].set_title('Tanh 函数')
axs[0, 1].grid(True)
axs[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axs[0, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)

axs[1, 0].plot(x, relu(x), 'b-', linewidth=2)
axs[1, 0].set_title('ReLU 函数')
axs[1, 0].grid(True)
axs[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axs[1, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)

axs[1, 1].plot(x, leaky_relu(x), 'm-', linewidth=2)
axs[1, 1].set_title('Leaky ReLU 函数 (α=0.01)')
axs[1, 1].grid(True)
axs[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axs[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)

# 绘制Softmax函数
softmax_output = softmax(x_softmax)
axs[2, 0].bar(range(len(x_softmax)), softmax_output, color='purple')
axs[2, 0].set_title('Softmax 函数示例')
axs[2, 0].set_xticks(range(len(x_softmax)))
axs[2, 0].set_xticklabels([f'{x:.1f}' for x in x_softmax])
axs[2, 0].set_ylabel('概率')
axs[2, 0].grid(True, axis='y')

# 绘制导数
axs[2, 1].plot(x, sigmoid_derivative(x), 'r-', label='Sigmoid导数', linewidth=2)
axs[2, 1].plot(x, tanh_derivative(x), 'g-', label='Tanh导数', linewidth=2)
axs[2, 1].plot(x, relu_derivative(x), 'b-', label='ReLU导数', linewidth=2)
axs[2, 1].plot(x, leaky_relu_derivative(x), 'm-', label='Leaky ReLU导数', linewidth=2)
axs[2, 1].set_title('激活函数的导数')
axs[2, 1].legend()
axs[2, 1].grid(True)
axs[2, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axs[2, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)
axs[2, 1].set_ylim(-0.1, 1.1)

plt.tight_layout()
plt.savefig('e:/Desktop/深度学习/第5章激活函数/activation_functions.png')
plt.show()
```

# 5. 激活函数的选择指南

选择合适的激活函数对神经网络的性能至关重要。以下是一些选择指南：

1. **隐藏层**：
   - 首选ReLU或其变体（如Leaky ReLU）
   - 对于循环神经网络，考虑使用Tanh或LSTM/GRU单元
   - 避免在深度网络中使用Sigmoid（梯度消失问题）

2. **输出层**：
   - 二分类问题：Sigmoid
   - 多分类问题：Softmax
   - 回归问题：线性激活（或无激活函数）

3. **特殊情况**：
   - 如果ReLU导致神经元死亡，尝试Leaky ReLU或ELU
   - 对于需要负值输出的情况，考虑Tanh或Leaky ReLU

# 6. 实际应用中的激活函数示例

## 6.1 使用PyTorch实现不同激活函数的神经网络

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 设置中文显示
plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体
plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 生成月牙形数据集
X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)
X = StandardScaler().fit_transform(X)  # 标准化数据

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换为PyTorch张量
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

# 定义不同激活函数的神经网络
class NeuralNetwork(nn.Module):
    def __init__(self, activation_function):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(2, 10)
        self.fc2 = nn.Linear(10, 10)
        self.fc3 = nn.Linear(10, 2)
        
        # 选择激活函数
        if activation_function == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation_function == 'tanh':
            self.activation = nn.Tanh()
        elif activation_function == 'relu':
            self.activation = nn.ReLU()
        elif activation_function == 'leaky_relu':
            self.activation = nn.LeakyReLU(0.01)
        else:
            raise ValueError("不支持的激活函数")
    
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)  # 输出层不使用激活函数，因为后面会用交叉熵损失
        return x

# 训练函数
def train_model(model, X_train, y_train, epochs=1000, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    losses = []
    
    for epoch in range(epochs):
        # 前向传播
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        if (epoch+1) % 100 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')
    
    return losses

# 评估函数
def evaluate_model(model, X_test, y_test):
    with torch.no_grad():
        outputs = model(X_test)
        _, predicted = torch.max(outputs.data, 1)
        accuracy = (predicted == y_test).sum().item() / y_test.size(0)
    return accuracy

# 可视化决策边界
def plot_decision_boundary(model, X, y, title):
    # 创建网格点
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    # 预测网格点的类别
    Z = np.zeros(xx.shape)
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            with torch.no_grad():
                logits = model(torch.FloatTensor([[xx[i, j], yy[i, j]]]))
                Z[i, j] = torch.argmax(logits, dim=1).item()
    
    # 绘制决策边界和数据点
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
    plt.title(title)
    plt.xlabel('特征1')
    plt.ylabel('特征2')
    plt.savefig(f'e:/Desktop/深度学习/第5章激活函数/decision_boundary_{title}.png')
    plt.show()

# 训练和评估不同激活函数的模型
activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
models = {}
losses = {}
accuracies = {}

for activation in activation_functions:
    print(f"\n训练使用 {activation} 激活函数的模型")
    model = NeuralNetwork(activation)
    loss_history = train_model(model, X_train_tensor, y_train_tensor)
    accuracy = evaluate_model(model, X_test_tensor, y_test_tensor)
    
    models[activation] = model
    losses[activation] = loss_history
    accuracies[activation] = accuracy
    
    print(f"{activation} 模型的测试准确率: {accuracy:.4f}")
    
    # 可视化决策边界
    plot_decision_boundary(model, X, y, f"{activation} 激活函数")

# 绘制损失曲线比较
plt.figure(figsize=(12, 8))
for activation, loss in losses.items():
    plt.plot(loss, label=activation)
plt.title('不同激活函数的损失曲线比较')
plt.xlabel('迭代次数')
plt.ylabel('损失')
plt.legend()
plt.grid(True)
plt.savefig('e:/Desktop/深度学习/第5章激活函数/loss_comparison.png')
plt.show()

# 绘制准确率比较
plt.figure(figsize=(10, 6))
plt.bar(accuracies.keys(), accuracies.values())
plt.title('不同激活函数的测试准确率比较')
plt.xlabel('激活函数')
plt.ylabel('准确率')
plt.ylim(0, 1)
for i, (activation, accuracy) in enumerate(accuracies.items()):
    plt.text(i, accuracy + 0.01, f'{accuracy:.4f}', ha='center')
plt.savefig('e:/Desktop/深度学习/第5章激活函数/accuracy_comparison.png')
plt.show()
```

# 7. 总结

激活函数是神经网络中不可或缺的组成部分，它们引入非线性因素，使网络能够学习复杂的模式。不同的激活函数有各自的优缺点：

- **Sigmoid**：早期常用，但存在梯度消失问题
- **Tanh**：改进的Sigmoid，输出以零为中心
- **ReLU**：计算高效，缓解梯度消失，但有神经元死亡问题
- **Leaky ReLU**：解决了ReLU的神经元死亡问题
- **Softmax**：适用于多分类问题的输出层

在实践中，ReLU及其变体因其简单高效而成为隐藏层的首选激活函数，而Sigmoid和Softmax则常用于输出层。选择合适的激活函数对神经网络的性能有显著影响，应根据具体任务和网络架构进行选择。
```

这个扩展版本包含了激活函数的详细数学原理、特性分析、代码实现和可视化效果。我添加了两个Python示例：一个用于可视化不同激活函数的形状和导数，另一个展示了不同激活函数在实际分类问题中的表现差异。所有图表都支持中文显示，并保存到指定目录。